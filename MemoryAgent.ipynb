{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Objectives\n",
        "- Use different message types Human message and AI message\n",
        "- Maintain a full conversation history using both message types\n",
        "- use gpt 4o model using langchain chatopen AI\n",
        "- create a sophisticated conversation loop\n",
        "\n",
        "#### Main goal - create a agent with memory"
      ],
      "metadata": {
        "id": "2baFQj5j9ZG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_groq langgraph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnmI3jhmqbqs",
        "outputId": "65cbbaee-7db6-431f-a200-31c8bed7d85e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_groq\n",
            "  Downloading langchain_groq-0.3.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting langgraph\n",
            "  Downloading langgraph-0.4.8-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.49 in /usr/local/lib/python3.11/dist-packages (from langchain_groq) (0.3.63)\n",
            "Collecting groq<1,>=0.4.1 (from langchain_groq)\n",
            "  Downloading groq-0.26.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting langgraph-checkpoint>=2.0.26 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.0.26-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting langgraph-prebuilt>=0.2.0 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.2.2-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting langgraph-sdk>=0.1.42 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.1.70-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.11.5)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain_groq) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain_groq) (0.28.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain_groq) (4.14.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.126 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain_groq) (0.3.44)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain_groq) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain_groq) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain_groq) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain_groq) (24.2)\n",
            "Collecting ormsgpack<2.0.0,>=1.8.0 (from langgraph-checkpoint>=2.0.26->langgraph)\n",
            "  Downloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk>=0.1.42->langgraph) (3.10.18)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (0.4.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain_groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.49->langchain_groq) (3.0.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.49->langchain_groq) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.49->langchain_groq) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.49->langchain_groq) (0.23.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.49->langchain_groq) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.49->langchain_groq) (2.4.0)\n",
            "Downloading langchain_groq-0.3.2-py3-none-any.whl (15 kB)\n",
            "Downloading langgraph-0.4.8-py3-none-any.whl (152 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.4/152.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groq-0.26.0-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.6/129.6 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.0.26-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.2.2-py3-none-any.whl (23 kB)\n",
            "Downloading langgraph_sdk-0.1.70-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ormsgpack, langgraph-sdk, groq, langgraph-checkpoint, langchain_groq, langgraph-prebuilt, langgraph\n",
            "Successfully installed groq-0.26.0 langchain_groq-0.3.2 langgraph-0.4.8 langgraph-checkpoint-2.0.26 langgraph-prebuilt-0.2.2 langgraph-sdk-0.1.70 ormsgpack-1.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "O2v4TOkQ81Q8"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict,List,Union\n",
        "from langgraph.graph import StateGraph,START,END\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.messages import HumanMessage,AIMessage"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "groq_api_key=userdata.get('groq_api')"
      ],
      "metadata": {
        "id": "sEUYPemG9CuG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agentstate(TypedDict):\n",
        "  message:List[Union[\"HumanMessage\",\"AIMessage\"]]"
      ],
      "metadata": {
        "id": "NWIbCHf4qgWb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGroq(groq_api_key=groq_api_key,\n",
        "               model_name=\"qwen-qwq-32b\")"
      ],
      "metadata": {
        "id": "dvzs4Nl--UwU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chatbot(state:Agentstate)->Agentstate:\n",
        "  response=llm.invoke(state['message'])\n",
        "  state['message'].append(AIMessage(content=response.content))\n",
        "  print(f\"AI : {state['message']}\")\n",
        "  print(f\"current_state  : {state['message']}\")\n",
        "\n",
        "  return state"
      ],
      "metadata": {
        "id": "25WV82WAXXKV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph=StateGraph(Agentstate)"
      ],
      "metadata": {
        "id": "hCEtebXR5XUA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph.add_node(\"chatbot\",chatbot)\n",
        "graph.add_edge(START,\"chatbot\")\n",
        "graph.add_edge(\"chatbot\",END)\n",
        "app=graph.compile()\n",
        "graph.compile()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "WznAkKVQ5km4",
        "outputId": "339246cd-28b9-4fa0-ab74-40a88d26739f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.CompiledStateGraph object at 0x7fa54aae5110>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversational_history=[]\n",
        "user_input=input(\"Enter: \")\n",
        "\n",
        "while user_input!=\"exit\":\n",
        "  conversational_history.append(HumanMessage(content=user_input))\n",
        "  result=app.invoke({'message':conversational_history})\n",
        "  # print(result['message'][-1].content.split(\"</think>\")[-1].strip())\n",
        "  conversational_history=result['message']\n",
        "  user_input=input(\"Enter: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOfyOOAB5uMx",
        "outputId": "a0018ca0-b55a-46ea-cfa1-4da941c850d9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter: hi\n",
            "AI : [HumanMessage(content='hi', additional_kwargs={}, response_metadata={}), AIMessage(content='\\n<think>\\nOkay, the user just said \"hi\". I should respond in a friendly way. Let me think of a simple greeting. Maybe \"Hello! How can I assist you today?\" That sounds good. It\\'s welcoming and opens the door for them to ask for help. I should make sure there\\'s no markdown, just plain text. Alright, that should work.\\n</think>\\n\\nHello! How can I assist you today?', additional_kwargs={}, response_metadata={})]\n",
            "current_state  : [HumanMessage(content='hi', additional_kwargs={}, response_metadata={}), AIMessage(content='\\n<think>\\nOkay, the user just said \"hi\". I should respond in a friendly way. Let me think of a simple greeting. Maybe \"Hello! How can I assist you today?\" That sounds good. It\\'s welcoming and opens the door for them to ask for help. I should make sure there\\'s no markdown, just plain text. Alright, that should work.\\n</think>\\n\\nHello! How can I assist you today?', additional_kwargs={}, response_metadata={})]\n",
            "Enter: my name is sam\n",
            "AI : [HumanMessage(content='hi', additional_kwargs={}, response_metadata={}), AIMessage(content='\\n<think>\\nOkay, the user just said \"hi\". I should respond in a friendly way. Let me think of a simple greeting. Maybe \"Hello! How can I assist you today?\" That sounds good. It\\'s welcoming and opens the door for them to ask for help. I should make sure there\\'s no markdown, just plain text. Alright, that should work.\\n</think>\\n\\nHello! How can I assist you today?', additional_kwargs={}, response_metadata={}), HumanMessage(content='my name is sam', additional_kwargs={}, response_metadata={}), AIMessage(content='\\n<think>\\nOkay, the user said \"my name is sam\". I need to respond appropriately. Let me start by acknowledging their name. I should say hello again and mention their name to make it personal. Then, maybe ask them how I can help them today. Keep it friendly and open-ended. Let me check if there\\'s anything else I need to consider. The previous messages were in English, so I\\'ll stick with that. No need for any markdown, just a simple response. Alright, that should work.\\n</think>\\n\\nHello Sam! How can I assist you today?', additional_kwargs={}, response_metadata={})]\n",
            "current_state  : [HumanMessage(content='hi', additional_kwargs={}, response_metadata={}), AIMessage(content='\\n<think>\\nOkay, the user just said \"hi\". I should respond in a friendly way. Let me think of a simple greeting. Maybe \"Hello! How can I assist you today?\" That sounds good. It\\'s welcoming and opens the door for them to ask for help. I should make sure there\\'s no markdown, just plain text. Alright, that should work.\\n</think>\\n\\nHello! How can I assist you today?', additional_kwargs={}, response_metadata={}), HumanMessage(content='my name is sam', additional_kwargs={}, response_metadata={}), AIMessage(content='\\n<think>\\nOkay, the user said \"my name is sam\". I need to respond appropriately. Let me start by acknowledging their name. I should say hello again and mention their name to make it personal. Then, maybe ask them how I can help them today. Keep it friendly and open-ended. Let me check if there\\'s anything else I need to consider. The previous messages were in English, so I\\'ll stick with that. No need for any markdown, just a simple response. Alright, that should work.\\n</think>\\n\\nHello Sam! How can I assist you today?', additional_kwargs={}, response_metadata={})]\n",
            "Enter: tlle me a joke\n",
            "AI : [HumanMessage(content='hi', additional_kwargs={}, response_metadata={}), AIMessage(content='\\n<think>\\nOkay, the user just said \"hi\". I should respond in a friendly way. Let me think of a simple greeting. Maybe \"Hello! How can I assist you today?\" That sounds good. It\\'s welcoming and opens the door for them to ask for help. I should make sure there\\'s no markdown, just plain text. Alright, that should work.\\n</think>\\n\\nHello! How can I assist you today?', additional_kwargs={}, response_metadata={}), HumanMessage(content='my name is sam', additional_kwargs={}, response_metadata={}), AIMessage(content='\\n<think>\\nOkay, the user said \"my name is sam\". I need to respond appropriately. Let me start by acknowledging their name. I should say hello again and mention their name to make it personal. Then, maybe ask them how I can help them today. Keep it friendly and open-ended. Let me check if there\\'s anything else I need to consider. The previous messages were in English, so I\\'ll stick with that. No need for any markdown, just a simple response. Alright, that should work.\\n</think>\\n\\nHello Sam! How can I assist you today?', additional_kwargs={}, response_metadata={}), HumanMessage(content='tlle me a joke', additional_kwargs={}, response_metadata={}), AIMessage(content='\\n<think>\\nOkay, the user is Sam and he asked for a joke. Let me think of a good one. I should make sure it\\'s appropriate and not too complicated.\\n\\nHmm, maybe something tech-related since I\\'m an AI? How about a computer joke? Let me see... Oh, the classic \"What do you call a fake noodle?\" joke. Wait, that\\'s \"An impasta.\" Yeah, that\\'s a good one. It\\'s simple and punny. \\n\\nWait, the user\\'s name is Sam. Maybe I can tailor the punchline to include his name for a personal touch. Let me try: \"Why did Sam the computer go to the doctor? Because it had a byte infection!\" \\n\\nWait, \"byte\" instead of \"bite\" infection. That\\'s a play on words related to computers. Does that make sense? The doctor joke structure works. Let me check if \"byte infection\" is clear. I think it\\'s okay. It\\'s a common pun. \\n\\nAlternatively, maybe the impasta joke is safer since it\\'s more general. But adding the user\\'s name could make it more engaging. Let me go with the Sam one. Let me also think of another one in case. \\n\\nOr maybe combine both? Start with the impasta as the main joke, then add the Sam one as a bonus. That way, there\\'s more content. \\n\\nWait, the user might prefer a short joke. Let me present both options. First, the impasta one, then the personalized one. That way Sam gets two jokes. \\n\\nAlso, check for any possible misunderstandings. The impasta is a pun on \"pasta\" and \"impostor.\" The byte infection is a play on \"byte\" and \"bite,\" which relates to computers. Both are clean and appropriate. \\n\\nAlright, let\\'s go with that. I\\'ll present them both to give Sam a good laugh.\\n</think>\\n\\nSure! Here\\'s a light-hearted one for you, Sam:  \\n\\n**Why did the computer go to the doctor?**  \\n*Because it had a **byte** infection!* 🤖😄  \\n\\n(And if you’re into food puns: *What do you call a fake noodle? An **impasta**!*)  \\n\\nNeed another one? Just ask!', additional_kwargs={}, response_metadata={})]\n",
            "current_state  : [HumanMessage(content='hi', additional_kwargs={}, response_metadata={}), AIMessage(content='\\n<think>\\nOkay, the user just said \"hi\". I should respond in a friendly way. Let me think of a simple greeting. Maybe \"Hello! How can I assist you today?\" That sounds good. It\\'s welcoming and opens the door for them to ask for help. I should make sure there\\'s no markdown, just plain text. Alright, that should work.\\n</think>\\n\\nHello! How can I assist you today?', additional_kwargs={}, response_metadata={}), HumanMessage(content='my name is sam', additional_kwargs={}, response_metadata={}), AIMessage(content='\\n<think>\\nOkay, the user said \"my name is sam\". I need to respond appropriately. Let me start by acknowledging their name. I should say hello again and mention their name to make it personal. Then, maybe ask them how I can help them today. Keep it friendly and open-ended. Let me check if there\\'s anything else I need to consider. The previous messages were in English, so I\\'ll stick with that. No need for any markdown, just a simple response. Alright, that should work.\\n</think>\\n\\nHello Sam! How can I assist you today?', additional_kwargs={}, response_metadata={}), HumanMessage(content='tlle me a joke', additional_kwargs={}, response_metadata={}), AIMessage(content='\\n<think>\\nOkay, the user is Sam and he asked for a joke. Let me think of a good one. I should make sure it\\'s appropriate and not too complicated.\\n\\nHmm, maybe something tech-related since I\\'m an AI? How about a computer joke? Let me see... Oh, the classic \"What do you call a fake noodle?\" joke. Wait, that\\'s \"An impasta.\" Yeah, that\\'s a good one. It\\'s simple and punny. \\n\\nWait, the user\\'s name is Sam. Maybe I can tailor the punchline to include his name for a personal touch. Let me try: \"Why did Sam the computer go to the doctor? Because it had a byte infection!\" \\n\\nWait, \"byte\" instead of \"bite\" infection. That\\'s a play on words related to computers. Does that make sense? The doctor joke structure works. Let me check if \"byte infection\" is clear. I think it\\'s okay. It\\'s a common pun. \\n\\nAlternatively, maybe the impasta joke is safer since it\\'s more general. But adding the user\\'s name could make it more engaging. Let me go with the Sam one. Let me also think of another one in case. \\n\\nOr maybe combine both? Start with the impasta as the main joke, then add the Sam one as a bonus. That way, there\\'s more content. \\n\\nWait, the user might prefer a short joke. Let me present both options. First, the impasta one, then the personalized one. That way Sam gets two jokes. \\n\\nAlso, check for any possible misunderstandings. The impasta is a pun on \"pasta\" and \"impostor.\" The byte infection is a play on \"byte\" and \"bite,\" which relates to computers. Both are clean and appropriate. \\n\\nAlright, let\\'s go with that. I\\'ll present them both to give Sam a good laugh.\\n</think>\\n\\nSure! Here\\'s a light-hearted one for you, Sam:  \\n\\n**Why did the computer go to the doctor?**  \\n*Because it had a **byte** infection!* 🤖😄  \\n\\n(And if you’re into food puns: *What do you call a fake noodle? An **impasta**!*)  \\n\\nNeed another one? Just ask!', additional_kwargs={}, response_metadata={})]\n",
            "Enter: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"me.txt\",'w') as file:\n",
        "  file.write(\"Your conversation Log:\\n\")\n",
        "  for message in conversational_history:\n",
        "    if isinstance(message,HumanMessage):\n",
        "      file.write(f\"Human: {message.content}\\n\")\n",
        "    else:\n",
        "      file.write(f\"AI: {message.content}\\n\\n\")\n",
        "  file.write(\"End of Conversation\")\n",
        "\n",
        "print(\"Conversation saved to me.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sl4pnGtGm-1M",
        "outputId": "1e2043fd-f27f-4388-8e79-4baa62fb7175"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conversation saved to me.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hAqvN7OJnPTe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}