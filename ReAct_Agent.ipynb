{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated,Sequence,TypedDict\n",
        "from langchain_core.messages import BaseMessage\n",
        "from langchain_core.messages import ToolMessage\n",
        "from langchain_core.messages import SystemMessage\n",
        "from langchain_core.tools import tool\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.graph import StateGraph,END\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langchain_groq import ChatGroq"
      ],
      "metadata": {
        "id": "gTEXpLTxrEFU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "groq_api_key=userdata.get('groq_api')"
      ],
      "metadata": {
        "id": "-561jvjXrxJ4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGroq(groq_api_key=groq_api_key,\n",
        "               model_name=\"deepseek-r1-distill-llama-70b\")"
      ],
      "metadata": {
        "id": "CPTAR0t0sHyi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[Sequence[BaseMessage], add_messages]"
      ],
      "metadata": {
        "id": "r_ls8DUnq-JG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tool\n",
        "def add(a: int, b:int):\n",
        "  \"\"\"This is an addition function that adds 2 numbers together\"\"\"\n",
        "\n",
        "  return a + b\n",
        "\n",
        "@tool\n",
        "def subtract(a: int, b: int):\n",
        "  \"\"\"Subtraction function\"\"\"\n",
        "  return a - b\n",
        "\n",
        "@tool\n",
        "def multiply(a: int, b: int):\n",
        "  \"\"\"Multiplication function\"\"\"\n",
        "  return a * b"
      ],
      "metadata": {
        "id": "WM4-qfLz0ZUW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [add, subtract, multiply]"
      ],
      "metadata": {
        "id": "8TMkvxJf1CrA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=llm.bind_tools(tools)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "dIbu80rl1KBi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_call(state:AgentState) -> AgentState:\n",
        "  system_prompt = SystemMessage(content=\n",
        "      \"You are my AI assistant, please answer my query to the best of your ability.\"\n",
        "  )\n",
        "  response = model.invoke([system_prompt] + state[\"messages\"])\n",
        "  return {\"messages\": [response]}"
      ],
      "metadata": {
        "id": "IUjEQWli1TE5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def should_continue(state: AgentState):\n",
        "  messages = state[\"messages\"]\n",
        "  last_message = messages[-1]\n",
        "  if not last_message.tool_calls:\n",
        "      return \"end\"\n",
        "  else:\n",
        "      return \"continue\""
      ],
      "metadata": {
        "id": "183Czd9T2BDG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph = StateGraph(AgentState)\n",
        "graph.add_node(\"our_agent\", model_call)\n",
        "\n",
        "\n",
        "tool_node = ToolNode(tools=tools)\n",
        "graph.add_node(\"tools\", tool_node)\n",
        "\n",
        "graph.set_entry_point(\"our_agent\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6cwIhvY2j0h",
        "outputId": "422e29d9-c651-4da3-95db-fe4213683c62"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7f8b974a5810>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "graph.add_conditional_edges(\n",
        "    \"our_agent\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\": \"tools\",\n",
        "        \"end\": END,\n",
        "    },\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v93sZH2Z3KuM",
        "outputId": "029f1e1f-fd88-4a09-9c3c-6d8936e39bad"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7f8b974a5810>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "graph.add_edge(\"tools\", \"our_agent\")\n",
        "\n",
        "app = graph.compile()\n"
      ],
      "metadata": {
        "id": "2IZdq7Qi3eQO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_stream(stream):\n",
        "  for s in stream:\n",
        "      message = s[\"messages\"][-1]\n",
        "      if isinstance(message, tuple):\n",
        "          print(message)\n",
        "      else:\n",
        "          message.pretty_print()"
      ],
      "metadata": {
        "id": "erp-j3u44B-B"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = {\"messages\": [(\"user\", \"Add 40 + 12 and then multiply the result by 6. Also tell me a joke please.\")]}\n",
        "print_stream(app.stream(inputs, stream_mode=\"values\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLow3NYf4dFN",
        "outputId": "5c6f4cd7-cfef-4180-88ba-3bc1ece18d6e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Add 40 + 12 and then multiply the result by 6. Also tell me a joke please.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "To solve the problem step by step:\n",
            "\n",
            "1. **Addition**: 40 + 12 = 52\n",
            "2. **Multiplication**: 52 Ã— 6 = 312\n",
            "\n",
            "**Answer**: 312\n",
            "\n",
            "**Joke**: Why don't skeletons fight each other? Because they don't have the guts! ðŸ˜„\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OJiNvML94euc"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}